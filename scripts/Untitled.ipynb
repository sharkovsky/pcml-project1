{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded training data: 250000 samples and 30 features\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = os.path.join( os.getcwd(), \"..\", \"data\", \"train.csv\")\n",
    "\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH, sub_sample=False)\n",
    "N_FEATURES=tX.shape[1]\n",
    "N_SAMPLES=tX.shape[0]\n",
    "print( \"loaded training data: %d samples and %d features\\n\"%(N_SAMPLES, N_FEATURES) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.]\n"
     ]
    }
   ],
   "source": [
    "y_colors = np.array(['b']*N_SAMPLES)\n",
    "y_colors[y==-1] = 'r'\n",
    "y += 1.\n",
    "y *= 0.5\n",
    "y_vals = np.unique(y)\n",
    "print(y_vals)\n",
    "# If I understood correctly, -1 are bosons, i.e. boson events will be colored in red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names=('DER_mass_MMC' , 'DER_mass_transverse_met_lep' , 'DER_mass_vis' , 'DER_pt_h' , 'DER_deltaeta_jet_jet' , 'DER_mass_jet_jet' , 'DER_prodeta_jet_jet' , 'DER_deltar_tau_lep' , 'DER_pt_tot' , 'DER_sum_pt' , 'DER_pt_ratio_lep_tau' , 'DER_met_phi_centrality' , 'DER_lep_eta_centrality' , 'PRI_tau_pt' , 'PRI_tau_eta' , 'PRI_tau_phi' , 'PRI_lep_pt' , 'PRI_lep_eta' , 'PRI_lep_phi' , 'PRI_met' , 'PRI_met_phi' , 'PRI_met_sumet' , 'PRI_jet_num' , 'PRI_jet_leading_pt' , 'PRI_jet_leading_eta' , 'PRI_jet_leading_phi' , 'PRI_jet_subleading_pt' , 'PRI_jet_subleading_eta' , 'PRI_jet_subleading_phi' , 'PRI_jet_all_pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_cols_plots=5\n",
    "n_rows_plots=int(N_FEATURES/n_cols_plots)+1\n",
    "print ( \"plotting a %d x %d matrix\" % (n_cols_plots, n_rows_plots))\n",
    "#n_rows_plots=1\n",
    "#f, ax_array = plt.subplots(n_rows_plots, n_cols_plots, sharex='col', sharey='row')\n",
    "f, ax_array = plt.subplots(n_rows_plots, n_cols_plots, figsize=(16, 16 ))\n",
    "\n",
    "for row in range(n_rows_plots):\n",
    "    axes = ax_array[row]\n",
    "    for col in range(n_cols_plots):\n",
    "        #ax = ax_array[row*n_cols_plots + col]\n",
    "        if row*n_cols_plots + col < tX.shape[1]:\n",
    "            ax = axes[col]\n",
    "            data_to_plot=tX[:,row*n_cols_plots + col]\n",
    "            ax.scatter(range(N_SAMPLES), data_to_plot, c=y_colors,alpha=0.2)\n",
    "        \n",
    "        \n",
    "#figpath=os.path.join( os.getcwd(), \"..\", \"figures\", \"scatterplots.pdf\")\n",
    "#plt.savefig(figpath)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181886\n"
     ]
    }
   ],
   "source": [
    "NaN_indices =  np.where( tX < -900.0 )[0] \n",
    "NaN_indices = np.unique(NaN_indices)\n",
    "print(len(NaN_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(68114, 30)\n"
     ]
    }
   ],
   "source": [
    "print(tX.shape)\n",
    "tX = np.delete(tX, NaN_indices, axis=0)\n",
    "y = np.delete(y, NaN_indices, axis=0)\n",
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_colors = np.delete(y_colors, NaN_indices, axis=0)\n",
    "N_SAMPLES=tX.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Primitive Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "primitive_features_indices=[index for index,name in enumerate(feature_names) if name[0:3]=='PRI']\n",
    "N_PRI_FEAT=len(primitive_features_indices)\n",
    "print(\"detected %d primitive features\"%(N_PRI_FEAT))\n",
    "#print(primitive_features_indices)\n",
    "#for index in primitive_features_indices:\n",
    "#    print(feature_names[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PRI_tX = []\n",
    "for index in primitive_features_indices:\n",
    "    PRI_tX.append(tX[:,index])\n",
    "PRI_tX = np.array(PRI_tX).T.reshape( (N_SAMPLES,N_PRI_FEAT) )\n",
    "#print(PRI_tX.shape)\n",
    "#print(tX[0:3,primitive_features_indices[0]])\n",
    "#print(tX[0:3,primitive_features_indices[1]])\n",
    "#print(PRI_tX[0:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_cols_plots=4\n",
    "n_rows_plots=int(N_PRI_FEAT/n_cols_plots)+1\n",
    "print ( \"plotting a %d x %d matrix\" % (n_cols_plots, n_rows_plots))\n",
    "#n_rows_plots=1\n",
    "#f, ax_array = plt.subplots(n_rows_plots, n_cols_plots, sharex='col', sharey='row')\n",
    "f, ax_array = plt.subplots(n_rows_plots, n_cols_plots, figsize=(16, 16 ))\n",
    "\n",
    "for row in range(n_rows_plots):\n",
    "    axes = ax_array[row]\n",
    "    for col in range(n_cols_plots):\n",
    "        #ax = ax_array[row*n_cols_plots + col]\n",
    "        if row*n_cols_plots + col < PRI_tX.shape[1]:\n",
    "            ax = axes[col]\n",
    "            data_to_plot=PRI_tX[:,row*n_cols_plots + col]\n",
    "            ax.scatter(range(N_SAMPLES), data_to_plot, c=y_colors,alpha=0.2)\n",
    "        \n",
    "        \n",
    "figpath=os.path.join( os.getcwd(), \"..\", \"figures\", \"scatterplots.pdf\")\n",
    "\n",
    "plt.savefig(figpath)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derived features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "der_features_indices=[index for index,name in enumerate(feature_names) if name[0:3]=='DER']\n",
    "N_DER_FEAT=len(der_features_indices)\n",
    "print(\"detected %d primitive features\"%(N_DER_FEAT))\n",
    "#print(primitive_features_indices)\n",
    "#for index in primitive_features_indices:\n",
    "#    print(feature_names[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DER_tX = []\n",
    "for index in der_features_indices:\n",
    "    DER_tX.append(tX[:,index])\n",
    "DER_tX = np.array(DER_tX).T.reshape( (N_SAMPLES,N_DER_FEAT) )\n",
    "#DERnt(DER_tX.shape)\n",
    "#DERnt(tX[0:3,DERmitive_features_indices[0]])\n",
    "#DERnt(tX[0:3,DERmitive_features_indices[1]])\n",
    "#DERnt(DER_tX[0:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_cols_plots=4\n",
    "n_rows_plots=int(N_DER_FEAT/n_cols_plots)+1\n",
    "print ( \"plotting a %d x %d matrix\" % (n_cols_plots, n_rows_plots))\n",
    "#n_rows_plots=1\n",
    "#f, ax_array = plt.subplots(n_rows_plots, n_cols_plots, sharex='col', sharey='row')\n",
    "f, ax_array = plt.subplots(n_rows_plots, n_cols_plots, figsize=(16, 16 ))\n",
    "\n",
    "for row in range(n_rows_plots):\n",
    "    axes = ax_array[row]\n",
    "    for col in range(n_cols_plots):\n",
    "        #ax = ax_array[row*n_cols_plots + col]\n",
    "        if row*n_cols_plots + col < DER_tX.shape[1]:\n",
    "            ax = axes[col]\n",
    "            data_to_plot=DER_tX[:,row*n_cols_plots + col]\n",
    "            ax.scatter(range(N_SAMPLES), data_to_plot, c=y_colors,alpha=0.2)\n",
    "        \n",
    "        \n",
    "figpath=os.path.join( os.getcwd(), \"..\", \"figures\", \"scatterplots.pdf\")\n",
    "\n",
    "plt.savefig(figpath)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove features that have \"missing data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for PRI data\n",
    "#good_indices = [x for x in range(9)]\n",
    "#good_indices.append(16)\n",
    "\n",
    "# for all data\n",
    "#good_indices = [1,2,3,7,8,9,10,11,13,14,15,16,17,18,19,20,21,29]\n",
    "\n",
    "\n",
    "# for DER data\n",
    "good_indices = [1,2,3,7,8,9,10,11]\n",
    "\n",
    "print(good_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for PRI data\n",
    "#red_tX = PRI_tX[:,good_indices]\n",
    "# for all data\n",
    "#red_tX = tX[:,good_indices]\n",
    "#for DER data\n",
    "#red_tX = 0.0*DER_tX[:,good_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whiten Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68114, 30)\n",
      "(30,)\n",
      "(30, 30)\n"
     ]
    }
   ],
   "source": [
    "to_whiten = tX\n",
    "U, s, Vt = np.linalg.svd(to_whiten, full_matrices=False)\n",
    "print(U.shape)\n",
    "print(s.shape)\n",
    "print(Vt.shape)\n",
    "N_WHIT_FEAT=s.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "whit_tX = U.dot(Vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_cols_plots=4\n",
    "n_rows_plots=int(N_WHIT_FEAT/n_cols_plots)+1\n",
    "print ( \"plotting a %d x %d matrix\" % (n_cols_plots, n_rows_plots))\n",
    "#n_rows_plots=1\n",
    "#f, ax_array = plt.subplots(n_rows_plots, n_cols_plots, sharex='col', sharey='row')\n",
    "f, ax_array = plt.subplots(n_rows_plots, n_cols_plots, figsize=(16, 16 ))\n",
    "\n",
    "for row in range(n_rows_plots):\n",
    "    axes = ax_array[row]\n",
    "    for col in range(n_cols_plots):\n",
    "        #ax = ax_array[row*n_cols_plots + col]\n",
    "        if row*n_cols_plots + col < whit_tX.shape[1]:\n",
    "            ax = axes[col]\n",
    "            data_to_plot=whit_tX[:,row*n_cols_plots + col]\n",
    "            ax.scatter(range(N_SAMPLES), data_to_plot, c=y_colors,alpha=0.2)\n",
    "        \n",
    "        \n",
    "figpath=os.path.join( os.getcwd(), \"..\", \"figures\", \"scatterplots.pdf\")\n",
    "\n",
    "plt.savefig(figpath)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1/(1+np.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    XX = tx.dot(w)\n",
    "    print(XX.shape)\n",
    "    LOG_part = np.log( 1 + np.exp( XX ) )\n",
    "    print(LOG_part.shape)\n",
    "    PROD_part = np.multiply(y, tx.dot(w) )\n",
    "    print(PROD_part.shape)\n",
    "    a = LOG_part - PROD_part\n",
    "    return np.sum( a )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return tx.T.dot( sigmoid(tx.dot(w)) - y.reshape((tx.shape[0],1)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    grad = calculate_gradient(y,tx, w)\n",
    "    w -= gamma*grad\n",
    "    \n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from helpers import de_standardize\n",
    "from plots import visualization\n",
    "\n",
    "def logistic_regression_gradient_descent_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    threshold = 1e-8\n",
    "    gamma = 0.001\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criteria\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    return losses, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "losses, w_star = logistic_regression_gradient_descent_demo(y, whit_tX)\n",
    "#print(w_star.shape)\n",
    "#print(w_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "ax=f.add_subplot(2,1,1)\n",
    "ax.scatter(whit_tX[:,0],whit_tX[:,1], c=y_colors, alpha=0.1)\n",
    "\n",
    "ax=f.add_subplot(2,1,2)\n",
    "prediction = sigmoid(whit_tX[:,0:1].dot(w_star[1:2]))\n",
    "prediction = prediction < 0.5\n",
    "#ax.plot(prediction)\n",
    "ax.scatter(whit_tX[:,0],whit_tX[:,1], c=prediction, alpha=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error = np.sum(np.abs(prediction - y.reshape(N_SAMPLES,1)))\n",
    "print(error)\n",
    "print(error/N_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_hessian(y, tx, w):\n",
    "    \"\"\"return the hessian of the loss function.\"\"\"\n",
    "    S1 = sigmoid(tx.dot(w))[:,0]\n",
    "    S1 = np.diag(S1)\n",
    "    #print(S1.shape)\n",
    "    S2 = 1.0 - sigmoid(tx.dot(w))\n",
    "    S2 = S2[:,0]\n",
    "    S2 = np.diag(S2)\n",
    "    #print(S2.shape)\n",
    "    S = S1.dot(S2)\n",
    "    return tx.T.dot(S.dot(tx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, w):\n",
    "    \"\"\"return the loss, gradient, and hessian.\"\"\"\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    grad = calculate_gradient(y,tx, w)\n",
    "    hess = calculate_hessian(y, tx, w)\n",
    "    return loss, grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning_by_newton_method(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step on Newton's method.\n",
    "    return the loss and updated w.\n",
    "    \"\"\"\n",
    "    loss, grad, hess = logistic_regression(y,tx,w)\n",
    "    w -= gamma * np.linalg.inv(hess).dot(grad);\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_newton_method_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 1000\n",
    "    gamma = 0.5\n",
    "    threshold = 1e-3\n",
    "    lambda_ = 0.1\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_newton_method(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 5 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criteria\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2])/np.abs(losses[-1]) < threshold:\n",
    "            print(\"[Exit Condition Met]: Current iteration={i}, the loss={l}\".format(i=iter, l=loss)) \n",
    "            break\n",
    "    return losses, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68114, 1)\n",
      "(68114, 1)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-044484293de5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_star\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression_newton_method_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhit_tX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-82ff15d62fa1>\u001b[0m in \u001b[0;36mlogistic_regression_newton_method_demo\u001b[0;34m(y, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# get loss and update w.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_by_newton_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m# log info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-7501676bcbfb>\u001b[0m in \u001b[0;36mlearning_by_newton_method\u001b[0;34m(y, tx, w, gamma)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mupdated\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-a0f48cb41eb5>\u001b[0m in \u001b[0;36mlogistic_regression\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"return the loss, gradient, and hessian.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mhess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_hessian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-5aef94f4d2ab>\u001b[0m in \u001b[0;36mcalculate_loss\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mLOG_part\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mXX\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOG_part\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mPROD_part\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPROD_part\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLOG_part\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mPROD_part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses, w_star = logistic_regression_newton_method_demo(y, whit_tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tX_pred = np.c_[np.ones((y.shape[0], 1)), whit_tX]\n",
    "\n",
    "f = plt.figure()\n",
    "ax=f.add_subplot(2,1,1)\n",
    "ax.scatter(tX[:,0],tX[:,1], c=y_colors, alpha=0.1)\n",
    "\n",
    "ax=f.add_subplot(2,1,2)\n",
    "prediction = sigmoid(tX_pred[:,0:1].dot(w_star[0:1]))\n",
    "prediction = np.array( [ int(x) for x in (prediction > 0.5)] )\n",
    "#ax.plot(prediction,'*')\n",
    "ax.scatter(tX[:,0],tX[:,1], c=prediction, alpha=0.1)\n",
    "\n",
    "prediction = sigmoid(tX_pred.dot(w_star))\n",
    "prediction = np.array( [ int(x) for x in (prediction > 0.5)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error = np.sum(np.abs(prediction - y))\n",
    "print(prediction)\n",
    "print(y)\n",
    "print(error)\n",
    "print(error/N_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
