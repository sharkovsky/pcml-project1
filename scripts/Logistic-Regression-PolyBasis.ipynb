{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**************\n",
    "# with NaNs\n",
    "# tX\n",
    "#tX_path=os.path.join( os.getcwd(), \"..\", \"data\", \"training-data-pca-with-dummy-vars-with-NaNs.csv\")\n",
    "#tX_path=os.path.join( os.getcwd(), \"..\", \"data\", \"training-data-whitened-with-dummy-vars-with-NaNs.csv\")\n",
    "#tX_path=os.path.join( os.getcwd(), \"..\", \"data\", \"training-data-standardized-with-dummy-vars-with-NaNs.csv\")\n",
    "#tX_path=os.path.join( os.getcwd(), \"..\", \"data\", \"training-data-standardized-with-dummy-vars-with-NaNs-removed-corr.csv\")\n",
    "#tX_path=os.path.join( os.getcwd(), \"..\", \"data\", \"training-data-whitened-using-PCA-with-dummy-vars-with-NaNs.csv\")\n",
    "tX_path=os.path.join( os.getcwd(), \"..\", \"data\", \"training-data-pca-with-dummy-vars-med-imput-NaNs.csv\")\n",
    "# y\n",
    "y_path=os.path.join( os.getcwd(), \"..\", \"data\", \"y-labels.csv\")\n",
    "\n",
    "\n",
    "#**************\n",
    "# no NaNs\n",
    "# tX\n",
    "#tX_path=os.path.join( os.getcwd(), \"..\", \"data\", \"training-data-standardized-with-dummy-vars-no-NaNs-removed-corr.csv\")\n",
    "# y\n",
    "#y_path=os.path.join( os.getcwd(), \"..\", \"data\", \"y-labels-no-NaNs.csv\")\n",
    "\n",
    "#tX = np.loadtxt(tX_path)\n",
    "#y = np.loadtxt(y_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250000\n",
      "250000\n",
      "(250000, 4)\n"
     ]
    }
   ],
   "source": [
    "N_SAMPLES = tX.shape[0]\n",
    "print(N_SAMPLES)\n",
    "print(y.shape[0])\n",
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 85667.0 boson events out of 250000 total events\n"
     ]
    }
   ],
   "source": [
    "y_colors = np.array(['b']*N_SAMPLES)\n",
    "y_colors[y==0] = 'r'\n",
    "print(\"Found {b} boson events out of {N} total events\".format(b=np.sum(y), N=N_SAMPLES))\n",
    "# If I understood correctly, 1 are bosons, i.e. boson events will be colored in blue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Polynomial Basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def multivar_poly_basis(x, degree):\n",
    "    n_feat = x.shape[1]\n",
    "    n_samp = x.shape[0]\n",
    "\n",
    "    #do a first iteration to avoid concatenating empty array problems\n",
    "    temp0 = x[0,:]\n",
    "    temp = x[0,:]\n",
    "    for deg in range(1,degree):\n",
    "        temp = np.outer(temp0,temp)\n",
    "        upper_tri_indices = np.triu_indices(n=temp.shape[0],m=temp.shape[1])\n",
    "        temp = temp[upper_tri_indices]\n",
    "        \n",
    "    x_ret = temp\n",
    "    \n",
    "    for sample in range(1,n_samp):\n",
    "        temp0 = x[sample,:].reshape(n_feat,1)\n",
    "        temp = x[sample,:].reshape(n_feat,1)\n",
    "#        print(temp.shape)\n",
    "        for deg in range(1,degree):\n",
    "            temp = np.outer(temp0,temp)\n",
    "            upper_tri_indices = np.triu_indices(n=temp.shape[0],m=temp.shape[1])\n",
    "            temp = temp[upper_tri_indices]\n",
    "        x_ret = np.r_[x_ret, temp]\n",
    "        \n",
    "    x_ret = x_ret.reshape(n_samp, int(x_ret.size/n_samp))\n",
    "    return x_ret\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  5 11]]\n",
      "----\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   8,   20,   44,   50,  110,  242,   50,  110,  125,  275,  605,\n",
       "         242,  275,  605, 1331]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test\n",
    "a = np.array([2,5,11]).reshape((1,3))\n",
    "print(a)\n",
    "print(\"----\")\n",
    "\n",
    "multivar_poly_basis(a,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1/(1+np.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    Xw = tx.dot(w)\n",
    "#    print(Xw.shape)\n",
    "#    print(y.shape)\n",
    "    LOG_part = np.log( 1 + np.exp( Xw ) )\n",
    "#    print(LOG_part.shape)\n",
    "    PROD_part = np.multiply(y.reshape(N_SAMPLES,1), Xw )\n",
    "#    print(PROD_part.shape)\n",
    "    a = LOG_part - PROD_part\n",
    "    return np.sum( a )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_gradient(y, tx, w,lambda_=0.):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return tx.T.dot( sigmoid(tx.dot(w)) - y.reshape((tx.shape[0],1)) ) + lambda_*w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    grad = calculate_gradient(y,tx, w)\n",
    "    w -= gamma*grad\n",
    "    \n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helpers import de_standardize\n",
    "from plots import visualization\n",
    "\n",
    "def logistic_regression_gradient_descent_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    threshold = 1e-8\n",
    "    gamma = 0.001\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criteria\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    return losses, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "losses, w_star = logistic_regression_gradient_descent_demo(y, whit_tX)\n",
    "#print(w_star.shape)\n",
    "#print(w_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "ax=f.add_subplot(2,1,1)\n",
    "ax.scatter(whit_tX[:,0],whit_tX[:,1], c=y_colors, alpha=0.1)\n",
    "\n",
    "ax=f.add_subplot(2,1,2)\n",
    "prediction = sigmoid(whit_tX[:,0:1].dot(w_star[1:2]))\n",
    "prediction = prediction < 0.5\n",
    "#ax.plot(prediction)\n",
    "ax.scatter(whit_tX[:,0],whit_tX[:,1], c=prediction, alpha=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error = np.sum(np.abs(prediction - y.reshape(N_SAMPLES,1)))\n",
    "print(error)\n",
    "print(error/N_SAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_hessian(y, tx, w):\n",
    "    \"\"\"return the hessian of the loss function.\"\"\"\n",
    "    S1 = sigmoid(tx.dot(w))\n",
    "    S1 = S1.reshape((N_SAMPLES,1))\n",
    "#    print(S1.shape)\n",
    "    S2 = 1.0 - sigmoid(tx.dot(w))\n",
    "#    print(S2.shape)\n",
    "    S = np.multiply(S1,S2)\n",
    "#    print(S.shape)\n",
    "    S = scipy.sparse.spdiags(S[:,0], 0, N_SAMPLES, N_SAMPLES)\n",
    "    return tx.T.dot(S.dot(tx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, w,lambda_):\n",
    "    \"\"\"return the loss, gradient, and hessian.\"\"\"\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    grad = calculate_gradient(y,tx, w,lambda_)\n",
    "    hess = calculate_hessian(y, tx, w)\n",
    "    return loss, grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning_by_newton_method(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step on Newton's method.\n",
    "    return the loss and updated w.\n",
    "    \"\"\"\n",
    "    loss, grad, hess = logistic_regression(y,tx,w,lambda_)\n",
    "    w -= gamma * np.linalg.inv(hess).dot(grad);\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_newton_method_demo(y, x, w_initial):\n",
    "    # init parameters\n",
    "    max_iter = 1000\n",
    "    gamma = 0.25\n",
    "    threshold = 1e-8\n",
    "    lambda_ = 0.9\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = w_initial\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_newton_method(y, tx, w, gamma, lambda_)\n",
    "        # log info\n",
    "        if iter % 5 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criteria\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2])/np.abs(losses[-1]) < threshold:\n",
    "            print(\"[Exit Condition Met]: Current iteration={i}, the loss={l}\".format(i=iter, l=loss)) \n",
    "            break\n",
    "    return losses, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=190558.58276489575\n",
      "Current iteration=5, the loss=155689.22516863496\n",
      "Current iteration=10, the loss=154096.73553226635\n",
      "Current iteration=15, the loss=154003.1957477467\n",
      "Current iteration=20, the loss=153997.84546961935\n",
      "Current iteration=25, the loss=153997.54200611537\n",
      "[Exit Condition Met]: Current iteration=29, the loss=153997.52548001002\n"
     ]
    }
   ],
   "source": [
    "w0 = np.random.rand(tX.shape[1]+1,1)\n",
    "losses, w_star = logistic_regression_newton_method_demo(y, tX, w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tX_pred = np.c_[np.ones((y.shape[0], 1)), tX]\n",
    "prediction = sigmoid(tX_pred.dot(w_star))\n",
    "prediction = np.array( [ int(x) for x in (prediction > 0.5)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x10c8e4160>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = plt.figure()\n",
    "ax=f.add_subplot(2,2,1)\n",
    "ax.scatter(tX[:,0],tX[:,1], c=y_colors, alpha=0.1)\n",
    "\n",
    "ax=f.add_subplot(2,2,2)\n",
    "ax.scatter(tX[:,2],tX[:,3], c=y_colors, alpha=0.1)\n",
    "\n",
    "prediction_colors = np.array(['b']*N_SAMPLES)\n",
    "prediction_colors[prediction==0] = 'r'\n",
    "\n",
    "ax=f.add_subplot(2,2,3)\n",
    "ax.scatter(tX[:,0],tX[:,1], c=prediction_colors, alpha=0.1)\n",
    "\n",
    "ax=f.add_subplot(2,2,4)\n",
    "ax.scatter(tX[:,2],tX[:,3], c=prediction_colors, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 0 0 0]\n",
      "[ 1.  0.  0. ...,  1.  0.  0.]\n",
      "79688.0\n",
      "0.318752\n"
     ]
    }
   ],
   "source": [
    "error = np.sum(np.abs(prediction - y))\n",
    "print(prediction)\n",
    "print(y)\n",
    "print(error)\n",
    "print(error/N_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
