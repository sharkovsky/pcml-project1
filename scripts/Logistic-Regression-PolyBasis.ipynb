{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**************\n",
    "# with NaNs\n",
    "# tX\n",
    "#tX_path=os.path.join( os.getcwd(), \"..\", \"data\", \"training-data-pca-with-dummy-vars-with-NaNs.csv\")\n",
    "#tX_path=os.path.join( os.getcwd(), \"..\", \"data\", \"training-data-whitened-with-dummy-vars-with-NaNs.csv\")\n",
    "#tX_path=os.path.join( os.getcwd(), \"..\", \"data\", \"training-data-standardized-with-dummy-vars-with-NaNs.csv\")\n",
    "#tX_path=os.path.join( os.getcwd(), \"..\", \"data\", \"training-data-standardized-with-dummy-vars-with-NaNs-removed-corr.csv\")\n",
    "#tX_path=os.path.join( os.getcwd(), \"..\", \"data\", \"training-data-whitened-using-PCA-with-dummy-vars-with-NaNs.csv\")\n",
    "tX_path=os.path.join( os.getcwd(), \"..\", \"data\", \"training-data-whitened-using-PCA-with-dummy-vars-med-imput-NaNs.csv\")\n",
    "# y\n",
    "y_path=os.path.join( os.getcwd(), \"..\", \"data\", \"y-labels.csv\")\n",
    "\n",
    "\n",
    "#**************\n",
    "# no NaNs\n",
    "# tX\n",
    "#tX_path=os.path.join( os.getcwd(), \"..\", \"data\", \"training-data-standardized-with-dummy-vars-no-NaNs-removed-corr.csv\")\n",
    "# y\n",
    "#y_path=os.path.join( os.getcwd(), \"..\", \"data\", \"y-labels-no-NaNs.csv\")\n",
    "\n",
    "tX = np.loadtxt(tX_path)\n",
    "y = np.loadtxt(y_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250000\n",
      "250000\n",
      "(250000, 31)\n"
     ]
    }
   ],
   "source": [
    "N_SAMPLES = tX.shape[0]\n",
    "print(N_SAMPLES)\n",
    "print(y.shape[0])\n",
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 85667.0 boson events out of 250000 total events\n"
     ]
    }
   ],
   "source": [
    "y_colors = np.array(['b']*N_SAMPLES)\n",
    "y_colors[y==0] = 'r'\n",
    "print(\"Found {b} boson events out of {N} total events\".format(b=np.sum(y), N=N_SAMPLES))\n",
    "# If I understood correctly, 1 are bosons, i.e. boson events will be colored in blue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Polynomial Basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def full_multivar_poly_basis_deg_two(x):\n",
    "    n_feat = x.shape[1]\n",
    "    n_samp = x.shape[0]\n",
    "\n",
    "    #do a first iteration to avoid concatenating empty array problems\n",
    "    temp0 = x[0,:]\n",
    "    temp = x[0,:]\n",
    "    temp = np.outer(temp0,temp)\n",
    "    upper_tri_indices = np.triu_indices(n=temp.shape[0],m=temp.shape[1])\n",
    "    temp = temp[upper_tri_indices]\n",
    "        \n",
    "    x_ret = temp\n",
    "    \n",
    "    for sample in range(1,n_samp):\n",
    "        temp0 = x[sample,:].reshape(n_feat,1)\n",
    "        temp = x[sample,:].reshape(n_feat,1)\n",
    "#        print(temp.shape)\n",
    "        temp = np.outer(temp0,temp)\n",
    "        upper_tri_indices = np.triu_indices(n=temp.shape[0],m=temp.shape[1])\n",
    "        temp = temp[upper_tri_indices]\n",
    "        x_ret = np.r_[x_ret, temp]\n",
    "        \n",
    "    x_ret = x_ret.reshape(n_samp, int(x_ret.size/n_samp))\n",
    "    return x_ret\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduced_multivar_poly_basis_deg_n(x, n):\n",
    "    if n < 3:\n",
    "        return np.array([]).reshape(x.shape[0],0)\n",
    "    return np.power(x,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_poly_basis(x,deg):\n",
    "    assert(deg > 0)\n",
    "    x_ret = np.c_[np.ones((x.shape[0],1)), x]\n",
    "    if deg <= 1:\n",
    "        return x_ret\n",
    "    \n",
    "    x_ret = np.c_[x_ret, full_multivar_poly_basis_deg_two(x)]\n",
    "    \n",
    "    if deg > 1 and deg <= 2:\n",
    "        return x_ret\n",
    "    \n",
    "    for d in range(3,deg+1):\n",
    "        x_ret = np.c_[x_ret, reduced_multivar_poly_basis_deg_n(x, d)]\n",
    "        \n",
    "    return x_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  5 11]\n",
      " [ 7  9 13]]\n",
      "----\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,   2.00000000e+00,   5.00000000e+00,\n",
       "          1.10000000e+01,   4.00000000e+00,   1.00000000e+01,\n",
       "          2.20000000e+01,   2.50000000e+01,   5.50000000e+01,\n",
       "          1.21000000e+02,   8.00000000e+00,   1.25000000e+02,\n",
       "          1.33100000e+03],\n",
       "       [  1.00000000e+00,   7.00000000e+00,   9.00000000e+00,\n",
       "          1.30000000e+01,   4.90000000e+01,   6.30000000e+01,\n",
       "          9.10000000e+01,   8.10000000e+01,   1.17000000e+02,\n",
       "          1.69000000e+02,   3.43000000e+02,   7.29000000e+02,\n",
       "          2.19700000e+03]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test\n",
    "a = np.array([2,5,11,7,9,13]).reshape((2,3))\n",
    "print(a)\n",
    "print(\"----\")\n",
    "\n",
    "my_poly_basis(a,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#tX_poly = my_poly_basis(tX,5)\n",
    "print(np.max(np.abs(tX)))\n",
    "degree = 7\n",
    "tX_poly = np.ones((tX.shape[0],1))\n",
    "for deg in range(1,degree+1):\n",
    "    tX_poly = np.c_[tX_poly, np.power(tX,deg)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "(250000, 218)\n"
     ]
    }
   ],
   "source": [
    "print(np.max(np.abs(tX_poly)))\n",
    "print(tX_poly.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1/(1+np.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    Xw = tx.dot(w)\n",
    "#    print(Xw)\n",
    "#    print(y.shape)\n",
    "    LOG_part = np.log( 1 + np.exp( Xw ) )\n",
    "#    print(LOG_part.shape)\n",
    "    PROD_part = np.multiply(y.reshape(N_SAMPLES,1), Xw )\n",
    "#    print(PROD_part.shape)\n",
    "    a = LOG_part - PROD_part\n",
    "    return np.sum( a )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_gradient(y, tx, w,lambda_=0.):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return tx.T.dot( sigmoid(tx.dot(w)) - y.reshape((tx.shape[0],1)) ) # - lambda_*w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    grad = calculate_gradient(y,tx, w)\n",
    "    w -= gamma*grad\n",
    "    \n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helpers import de_standardize\n",
    "from plots import visualization\n",
    "\n",
    "def logistic_regression_gradient_descent_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    threshold = 1e-8\n",
    "    gamma = 0.001\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criteria\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    return losses, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "losses, w_star = logistic_regression_gradient_descent_demo(y, whit_tX)\n",
    "#print(w_star.shape)\n",
    "#print(w_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "ax=f.add_subplot(2,1,1)\n",
    "ax.scatter(whit_tX[:,0],whit_tX[:,1], c=y_colors, alpha=0.1)\n",
    "\n",
    "ax=f.add_subplot(2,1,2)\n",
    "prediction = sigmoid(whit_tX[:,0:1].dot(w_star[1:2]))\n",
    "prediction = prediction < 0.5\n",
    "#ax.plot(prediction)\n",
    "ax.scatter(whit_tX[:,0],whit_tX[:,1], c=prediction, alpha=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error = np.sum(np.abs(prediction - y.reshape(N_SAMPLES,1)))\n",
    "print(error)\n",
    "print(error/N_SAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_hessian(y, tx, w):\n",
    "    \"\"\"return the hessian of the loss function.\"\"\"\n",
    "    S1 = sigmoid(tx.dot(w))\n",
    "    S1 = S1.reshape((N_SAMPLES,1))\n",
    "#    print(S1.shape)\n",
    "    S2 = 1.0 - sigmoid(tx.dot(w))\n",
    "#    print(S2.shape)\n",
    "    S = np.multiply(S1,S2)\n",
    "#    print(S.shape)\n",
    "    S = scipy.sparse.spdiags(S[:,0], 0, N_SAMPLES, N_SAMPLES)\n",
    "    return tx.T.dot(S.dot(tx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, w,lambda_):\n",
    "    \"\"\"return the loss, gradient, and hessian.\"\"\"\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    grad = calculate_gradient(y,tx, w,lambda_)\n",
    "    hess = calculate_hessian(y, tx, w)\n",
    "    return loss, grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning_by_newton_method(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step on Newton's method.\n",
    "    return the loss and updated w.\n",
    "    \"\"\"\n",
    "    loss, grad, hess = logistic_regression(y,tx,w,lambda_)\n",
    "    w -= gamma * np.linalg.inv(hess).dot(grad);\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_newton_method_demo(y, tx, w_initial):\n",
    "    # init parameters\n",
    "    max_iter = 5000\n",
    "    gamma = 0.2e-3\n",
    "    threshold = 1e-8\n",
    "    lambda_ = 0.1\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    #tx = np.c_[np.ones((y.shape[0], 1)), x] #CAREFUL: poly basis already puts in the ones\n",
    "    w = w_initial\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_newton_method(y, tx, w, gamma, lambda_)\n",
    "        # log info\n",
    "        if iter % 25 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criteria\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2])/np.abs(losses[-1]) < threshold:\n",
    "            print(\"[Exit Condition Met]: Current iteration={i}, the loss={l}\".format(i=iter, l=loss)) \n",
    "            break\n",
    "    return losses, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=215738.95177430558\n",
      "Current iteration=25, the loss=214467.69967754293\n",
      "Current iteration=50, the loss=213231.75735038\n",
      "Current iteration=75, the loss=212026.5644230777\n",
      "Current iteration=100, the loss=210849.51758413753\n",
      "Current iteration=125, the loss=209698.65168626318\n",
      "Current iteration=150, the loss=208572.36993077368\n",
      "Current iteration=175, the loss=207469.3256492951\n",
      "Current iteration=200, the loss=206388.35513562823\n",
      "Current iteration=225, the loss=205328.4345026383\n",
      "Current iteration=250, the loss=204288.65007070877\n",
      "Current iteration=275, the loss=203268.17716684073\n",
      "Current iteration=300, the loss=202266.26447017788\n",
      "Current iteration=325, the loss=201282.22216529297\n",
      "Current iteration=350, the loss=200315.4127880863\n",
      "Current iteration=375, the loss=199365.2440221882\n",
      "Current iteration=400, the loss=198431.1629365646\n",
      "Current iteration=425, the loss=197512.6513070036\n",
      "Current iteration=450, the loss=196609.22176563853\n",
      "Current iteration=475, the loss=195720.4145920811\n",
      "Current iteration=500, the loss=194845.7950089052\n",
      "Current iteration=525, the loss=193984.9508780945\n",
      "Current iteration=550, the loss=193137.49072056383\n",
      "Current iteration=575, the loss=192303.04199858216\n",
      "Current iteration=600, the loss=191481.24961454104\n",
      "Current iteration=625, the loss=190671.77458956564\n",
      "Current iteration=650, the loss=189874.29289279893\n",
      "Current iteration=675, the loss=189088.49439821107\n",
      "Current iteration=700, the loss=188314.08195003975\n",
      "Current iteration=725, the loss=187550.77052147087\n",
      "Current iteration=750, the loss=186798.28645401014\n",
      "Current iteration=775, the loss=186056.3667668997\n",
      "Current iteration=800, the loss=185324.75852787707\n",
      "Current iteration=825, the loss=184603.21827784553\n",
      "Current iteration=850, the loss=183891.51150319108\n",
      "Current iteration=875, the loss=183189.41215038428\n",
      "Current iteration=900, the loss=182496.70217828444\n",
      "Current iteration=925, the loss=181813.1711442068\n",
      "Current iteration=950, the loss=181138.61582023802\n",
      "Current iteration=975, the loss=180472.83983693662\n",
      "Current iteration=1000, the loss=179815.65335170532\n",
      "Current iteration=1025, the loss=179166.87273957414\n",
      "Current iteration=1050, the loss=178526.32030439153\n",
      "Current iteration=1075, the loss=177893.8240085732\n",
      "Current iteration=1100, the loss=177269.21721987994\n",
      "Current iteration=1125, the loss=176652.33847376268\n",
      "Current iteration=1150, the loss=176043.0312500111\n",
      "Current iteration=1175, the loss=175441.14376261452\n",
      "Current iteration=1200, the loss=174846.52876172695\n",
      "Current iteration=1225, the loss=174259.04334688763\n",
      "Current iteration=1250, the loss=173678.54879063022\n",
      "Current iteration=1275, the loss=173104.91037170278\n",
      "Current iteration=1300, the loss=172537.9972172633\n",
      "Current iteration=1325, the loss=171977.68215337253\n",
      "Current iteration=1350, the loss=171423.8415632385\n",
      "Current iteration=1375, the loss=170876.35525266384\n",
      "Current iteration=1400, the loss=170335.10632226392\n",
      "Current iteration=1425, the loss=169799.98104592494\n",
      "Current iteration=1450, the loss=169270.86875520667\n",
      "Current iteration=1475, the loss=168747.66172921378\n",
      "Current iteration=1500, the loss=168230.25508963704\n",
      "Current iteration=1525, the loss=167718.54670066814\n",
      "Current iteration=1550, the loss=167212.4370734185\n",
      "Current iteration=1575, the loss=166711.8292746549\n",
      "Current iteration=1600, the loss=166216.62883952342\n",
      "Current iteration=1625, the loss=165726.74368807182\n",
      "Current iteration=1650, the loss=165242.08404533504\n",
      "Current iteration=1675, the loss=164762.5623648227\n",
      "Current iteration=1700, the loss=164288.093255131\n",
      "Current iteration=1725, the loss=163818.59340960594\n",
      "Current iteration=1750, the loss=163353.9815388609\n",
      "Current iteration=1775, the loss=162894.17830598156\n",
      "Current iteration=1800, the loss=162439.1062642987\n",
      "Current iteration=1825, the loss=161988.68979762393\n",
      "Current iteration=1850, the loss=161542.8550627954\n",
      "Current iteration=1875, the loss=161101.5299344398\n",
      "Current iteration=1900, the loss=160664.6439518479\n",
      "Current iteration=1925, the loss=160232.12826788353\n",
      "Current iteration=1950, the loss=159803.91559979296\n",
      "Current iteration=1975, the loss=159379.94018187348\n",
      "Current iteration=2000, the loss=158960.13771988414\n",
      "Current iteration=2025, the loss=158544.44534713836\n",
      "Current iteration=2050, the loss=158132.80158218165\n",
      "Current iteration=2075, the loss=157725.14628797077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/checco/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:6: RuntimeWarning: overflow encountered in exp\n",
      "/home/checco/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:22: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=2100, the loss=inf\n",
      "Current iteration=2125, the loss=inf\n",
      "Current iteration=2150, the loss=inf\n",
      "Current iteration=2175, the loss=inf\n",
      "Current iteration=2200, the loss=inf\n",
      "Current iteration=2225, the loss=inf\n",
      "Current iteration=2250, the loss=inf\n",
      "Current iteration=2275, the loss=inf\n",
      "Current iteration=2300, the loss=inf\n",
      "Current iteration=2325, the loss=inf\n",
      "Current iteration=2350, the loss=inf\n",
      "Current iteration=2375, the loss=inf\n",
      "Current iteration=2400, the loss=inf\n",
      "Current iteration=2425, the loss=inf\n",
      "Current iteration=2450, the loss=inf\n",
      "Current iteration=2475, the loss=inf\n",
      "Current iteration=2500, the loss=inf\n",
      "Current iteration=2525, the loss=inf\n",
      "Current iteration=2550, the loss=inf\n",
      "Current iteration=2575, the loss=inf\n",
      "Current iteration=2600, the loss=inf\n",
      "Current iteration=2625, the loss=inf\n",
      "Current iteration=2650, the loss=inf\n",
      "Current iteration=2675, the loss=inf\n",
      "Current iteration=2700, the loss=inf\n",
      "Current iteration=2725, the loss=inf\n",
      "Current iteration=2750, the loss=inf\n",
      "Current iteration=2775, the loss=inf\n",
      "Current iteration=2800, the loss=inf\n",
      "Current iteration=2825, the loss=inf\n",
      "Current iteration=2850, the loss=inf\n",
      "Current iteration=2875, the loss=inf\n",
      "Current iteration=2900, the loss=inf\n",
      "Current iteration=2925, the loss=inf\n",
      "Current iteration=2950, the loss=inf\n",
      "Current iteration=2975, the loss=inf\n",
      "Current iteration=3000, the loss=inf\n",
      "Current iteration=3025, the loss=inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/checco/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=3050, the loss=inf\n",
      "Current iteration=3075, the loss=inf\n",
      "Current iteration=3100, the loss=inf\n",
      "Current iteration=3125, the loss=inf\n",
      "Current iteration=3150, the loss=inf\n",
      "Current iteration=3175, the loss=inf\n",
      "Current iteration=3200, the loss=inf\n",
      "Current iteration=3225, the loss=inf\n",
      "Current iteration=3250, the loss=inf\n",
      "Current iteration=3275, the loss=inf\n",
      "Current iteration=3300, the loss=inf\n",
      "Current iteration=3325, the loss=inf\n",
      "Current iteration=3350, the loss=inf\n",
      "Current iteration=3375, the loss=inf\n",
      "Current iteration=3400, the loss=inf\n",
      "Current iteration=3425, the loss=inf\n",
      "Current iteration=3450, the loss=inf\n",
      "Current iteration=3475, the loss=inf\n",
      "Current iteration=3500, the loss=inf\n",
      "Current iteration=3525, the loss=inf\n",
      "Current iteration=3550, the loss=inf\n",
      "Current iteration=3575, the loss=inf\n",
      "Current iteration=3600, the loss=inf\n",
      "Current iteration=3625, the loss=inf\n",
      "Current iteration=3650, the loss=inf\n",
      "Current iteration=3675, the loss=inf\n",
      "Current iteration=3700, the loss=inf\n",
      "Current iteration=3725, the loss=inf\n",
      "Current iteration=3750, the loss=inf\n",
      "Current iteration=3775, the loss=inf\n",
      "Current iteration=3800, the loss=inf\n",
      "Current iteration=3825, the loss=inf\n",
      "Current iteration=3850, the loss=inf\n",
      "Current iteration=3875, the loss=inf\n",
      "Current iteration=3900, the loss=inf\n",
      "Current iteration=3925, the loss=inf\n",
      "Current iteration=3950, the loss=inf\n",
      "Current iteration=3975, the loss=inf\n",
      "Current iteration=4000, the loss=inf\n",
      "Current iteration=4025, the loss=inf\n",
      "Current iteration=4050, the loss=inf\n",
      "Current iteration=4075, the loss=inf\n",
      "Current iteration=4100, the loss=inf\n",
      "Current iteration=4125, the loss=inf\n",
      "Current iteration=4150, the loss=inf\n",
      "Current iteration=4175, the loss=inf\n",
      "Current iteration=4200, the loss=inf\n",
      "Current iteration=4225, the loss=inf\n",
      "Current iteration=4250, the loss=inf\n",
      "Current iteration=4275, the loss=inf\n",
      "Current iteration=4300, the loss=inf\n",
      "Current iteration=4325, the loss=inf\n",
      "Current iteration=4350, the loss=inf\n",
      "Current iteration=4375, the loss=inf\n",
      "Current iteration=4400, the loss=inf\n",
      "Current iteration=4425, the loss=inf\n",
      "Current iteration=4450, the loss=inf\n",
      "Current iteration=4475, the loss=inf\n",
      "Current iteration=4500, the loss=inf\n",
      "Current iteration=4525, the loss=inf\n",
      "Current iteration=4550, the loss=inf\n",
      "Current iteration=4575, the loss=inf\n",
      "Current iteration=4600, the loss=inf\n",
      "Current iteration=4625, the loss=inf\n",
      "Current iteration=4650, the loss=inf\n",
      "Current iteration=4675, the loss=inf\n",
      "Current iteration=4700, the loss=inf\n",
      "Current iteration=4725, the loss=inf\n",
      "Current iteration=4750, the loss=inf\n",
      "Current iteration=4775, the loss=inf\n",
      "Current iteration=4800, the loss=inf\n",
      "Current iteration=4825, the loss=inf\n",
      "Current iteration=4850, the loss=inf\n",
      "Current iteration=4875, the loss=inf\n",
      "Current iteration=4900, the loss=inf\n",
      "Current iteration=4925, the loss=inf\n",
      "Current iteration=4950, the loss=inf\n",
      "Current iteration=4975, the loss=inf\n"
     ]
    }
   ],
   "source": [
    "#w0 = np.random.rand(tX_poly.shape[1]+1,1)\n",
    "w0 = np.random.rand(tX_poly.shape[1],1)\n",
    "losses, w_star = logistic_regression_newton_method_demo(y, tX_poly, w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tX_pred = np.c_[np.ones((y.shape[0], 1)), tX]\n",
    "tX_pred = tX_poly\n",
    "prediction = sigmoid(tX_pred.dot(w_star))\n",
    "prediction = np.array( [ int(x) for x in (prediction > 0.5)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "ax=f.add_subplot(2,2,1)\n",
    "ax.scatter(tX[:,0],tX[:,1], c=y_colors, alpha=0.1)\n",
    "\n",
    "ax=f.add_subplot(2,2,2)\n",
    "ax.scatter(tX[:,2],tX[:,3], c=y_colors, alpha=0.1)\n",
    "\n",
    "prediction_colors = np.array(['b']*N_SAMPLES)\n",
    "prediction_colors[prediction==0] = 'r'\n",
    "\n",
    "ax=f.add_subplot(2,2,3)\n",
    "ax.scatter(tX[:,0],tX[:,1], c=prediction_colors, alpha=0.1)\n",
    "\n",
    "ax=f.add_subplot(2,2,4)\n",
    "ax.scatter(tX[:,2],tX[:,3], c=prediction_colors, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 0 0 0]\n",
      "[ 1.  0.  0. ...,  1.  0.  0.]\n",
      "49238.0\n",
      "0.196952\n"
     ]
    }
   ],
   "source": [
    "error = np.sum(np.abs(prediction - y))\n",
    "print(prediction)\n",
    "print(y)\n",
    "print(error)\n",
    "print(error/N_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
