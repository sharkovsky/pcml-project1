{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required ML methods\n",
    "\n",
    "1. *** least_squares_GD (y, tx, inital_w, gamma, max_iters) ***\n",
    "Linear regression using gradient descent \n",
    "\n",
    "2. *** least_squares_SGD(y, tx, initial_w, gamma, max_iters) ***\n",
    "Linear regression using stochastic gradient descent \n",
    "\n",
    "3. *** least_squares(y, tx) ***\n",
    "Least squares regression using normal equations\n",
    "\n",
    "4. *** ridge_regression(y, tx, lambda_) ***\n",
    "Ridge regression using normal equations\n",
    "\n",
    "5. *** logistic_regression(y, tx, initial_w, gamma, max_iters) ***\n",
    "Logistic regression using gradient descent or SGD \n",
    "\n",
    "6. *** reg_logistic_regression(y, tx, lambda_, initial_w, gamma, max_iters) ***\n",
    "Regularized  logistic  regression  using  gradient  descent or SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import math\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from helpers import *\n",
    "\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# General gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# general gradient descent\n",
    "def gradient_descent(y, tx, initial_x, gamma, max_iters, compute_gradient, compute_loss):\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        w = w - gamma * gradient\n",
    "        \n",
    "        losses.append(loss)\n",
    "    \n",
    "    return w, losses[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma,\n",
    "                                seed, compute_gradient, compute_loss):\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    num_batches = math.floor(y.shape[0] / batch_size) \n",
    "    batches = batch_iter(y, tx, seed, batch_size, num_batches)\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        s_y, s_tx = next(batches)\n",
    "        gradient = compute_gradient(s_y, s_tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        # print(loss)\n",
    "        w = w - gamma * gradient\n",
    "        \n",
    "        losses.append(loss)\n",
    "        \n",
    "    return w, losses[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: perhaps import it instead of copy?\n",
    "def calculate_mse(e):\n",
    "    \"\"\"Calculate the mse for vector e.\"\"\"\n",
    "    return 1/2*np.mean(e**2)\n",
    "\n",
    "def calculate_mae(e):\n",
    "    \"\"\"Calculate the mae for vector e.\"\"\"\n",
    "    return np.mean(np.abs(e))\n",
    "\n",
    "def compute_loss_mse(y, tx, w):\n",
    "    \"\"\"Calculate the loss using mse \"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    return calculate_mse(e)\n",
    "\n",
    "def compute_gradient_mse(y, tx, w):\n",
    "    \"\"\" compute the gradient associated to the MSE cost function\"\"\"\n",
    "    e = y - (tx @ w)\n",
    "    return -1/y.shape[0] * (tx.T @ e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, gamma, max_iters):\n",
    "    return gradient_descent(y, tx, initial_w, gamma, max_iters,\n",
    "                            compute_gradient_mse, compute_loss_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iters = 1000\n",
    "gamma = 1e-7\n",
    "initial_w = np.zeros(tX.shape[1])\n",
    "\n",
    "w_lr_gd, loss_lr_gd = least_squares_GD(y, tX, initial_w, gamma, max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39938606005358546"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_lr_gd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using stochastic gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, gamma, max_iters):\n",
    "    batch_size = y.shape[0]//2\n",
    "    seed = 3\n",
    "    \n",
    "    return stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma,\n",
    "                                       seed, compute_gradient_mse, compute_loss_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iters = 1000\n",
    "gamma = 1e-8\n",
    "initial_w = np.zeros(tX.shape[1])\n",
    "\n",
    "w_lr_sgd, loss_lr_sgd = least_squares_SGD(y, tX, initial_w, gamma, max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41671005622007445"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_lr_sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    w = (np.linalg.inv(tx.T @ tx) @ tx.T @ y)\n",
    "    loss = compute_loss_mse(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_ls, loss_ls = least_squares(y, tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33968680990826089"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    w = np.linalg.inv(tx.T @ tx + lambda_ * np.identity(tx.shape[1])) @ tx.T @ y\n",
    "    loss = compute_loss_mse(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lamb = 23\n",
    "w_rr, loss_rr = ridge_regression(y, tX, lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33968793894774219"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3k]",
   "language": "python",
   "name": "conda-env-py3k-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
