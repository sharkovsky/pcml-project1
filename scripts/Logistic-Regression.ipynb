{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#**************\n",
    "# with NaNs\n",
    "# tX\n",
    "#tX_path=os.path.join( os.getcwd(), \"..\", \"data\", \"training-data-pca-with-dummy-vars-with-NaNs.csv\")\n",
    "#tX_path=os.path.join( os.getcwd(), \"..\", \"data\", \"training-data-whitened-with-dummy-vars-with-NaNs.csv\")\n",
    "#tX_path=os.path.join( os.getcwd(), \"..\", \"data\", \"training-data-standardized-with-dummy-vars-with-NaNs.csv\")\n",
    "#tX_path=os.path.join( os.getcwd(), \"..\", \"data\", \"training-data-standardized-with-dummy-vars-with-NaNs-removed-corr.csv\")\n",
    "tX_path=os.path.join( os.getcwd(), \"..\", \"data\", \"training-data-whitened-using-PCA-with-dummy-vars-with-NaNs.csv\")\n",
    "# y\n",
    "y_path=os.path.join( os.getcwd(), \"..\", \"data\", \"y-labels.csv\")\n",
    "\n",
    "\n",
    "#**************\n",
    "# no NaNs\n",
    "# tX\n",
    "#tX_path=os.path.join( os.getcwd(), \"..\", \"data\", \"training-data-standardized-with-dummy-vars-no-NaNs-removed-corr.csv\")\n",
    "# y\n",
    "#y_path=os.path.join( os.getcwd(), \"..\", \"data\", \"y-labels-no-NaNs.csv\")\n",
    "\n",
    "tX = np.loadtxt(tX_path)\n",
    "y = np.loadtxt(y_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250000\n",
      "250000\n",
      "(250000, 31)\n"
     ]
    }
   ],
   "source": [
    "N_SAMPLES = tX.shape[0]\n",
    "print(N_SAMPLES)\n",
    "print(y.shape[0])\n",
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 85667.0 boson events out of 250000 total events\n"
     ]
    }
   ],
   "source": [
    "y_colors = np.array(['b']*N_SAMPLES)\n",
    "y_colors[y==0] = 'r'\n",
    "print(\"Found {b} boson events out of {N} total events\".format(b=np.sum(y), N=N_SAMPLES))\n",
    "# If I understood correctly, 1 are bosons, i.e. boson events will be colored in blue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1/(1+np.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    Xw = tx.dot(w)\n",
    "#    print(Xw.shape)\n",
    "#    print(y.shape)\n",
    "    LOG_part = np.log( 1 + np.exp( Xw ) )\n",
    "#    print(LOG_part.shape)\n",
    "    PROD_part = np.multiply(y.reshape(N_SAMPLES,1), Xw )\n",
    "#    print(PROD_part.shape)\n",
    "    a = LOG_part - PROD_part\n",
    "    return np.sum( a )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return tx.T.dot( sigmoid(tx.dot(w)) - y.reshape((tx.shape[0],1)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    grad = calculate_gradient(y,tx, w)\n",
    "    w -= gamma*grad\n",
    "    \n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from helpers import standardize\n",
    "#from matplots import visualization\n",
    "\n",
    "def logistic_regression_gradient_descent_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    threshold = 1e-8\n",
    "    gamma = 0.001\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criteria\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    return losses, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "losses, w_star = logistic_regression_gradient_descent_demo(y, whit_tX)\n",
    "#print(w_star.shape)\n",
    "#print(w_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "ax=f.add_subplot(2,1,1)\n",
    "ax.scatter(whit_tX[:,0],whit_tX[:,1], c=y_colors, alpha=0.1)\n",
    "\n",
    "ax=f.add_subplot(2,1,2)\n",
    "prediction = sigmoid(whit_tX[:,0:1].dot(w_star[1:2]))\n",
    "prediction = prediction < 0.5\n",
    "#ax.plot(prediction)\n",
    "ax.scatter(whit_tX[:,0],whit_tX[:,1], c=prediction, alpha=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error = np.sum(np.abs(prediction - y.reshape(N_SAMPLES,1)))\n",
    "print(error)\n",
    "print(error/N_SAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_hessian(y, tx, w):\n",
    "    \"\"\"return the hessian of the loss function.\"\"\"\n",
    "    S1 = sigmoid(tx.dot(w))\n",
    "    S1 = S1.reshape((N_SAMPLES,1))\n",
    "#    print(S1.shape)\n",
    "    S2 = 1.0 - sigmoid(tx.dot(w))\n",
    "#    print(S2.shape)\n",
    "    S = np.multiply(S1,S2)\n",
    "#    print(S.shape)\n",
    "    S = scipy.sparse.spdiags(S[:,0], 0, N_SAMPLES, N_SAMPLES)\n",
    "    return tx.T.dot(S.dot(tx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, w):\n",
    "    \"\"\"return the loss, gradient, and hessian.\"\"\"\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    grad = calculate_gradient(y,tx, w)\n",
    "    hess = calculate_hessian(y, tx, w)\n",
    "    return loss, grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning_by_newton_method(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step on Newton's method.\n",
    "    return the loss and updated w.\n",
    "    \"\"\"\n",
    "    loss, grad, hess = logistic_regression(y,tx,w)\n",
    "    w -= gamma * np.linalg.inv(hess).dot(grad);\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_newton_method_demo(y, x, w_initial):\n",
    "    # init parameters\n",
    "    max_iter = 1000\n",
    "    gamma = 0.1\n",
    "    threshold = 1e-18\n",
    "    lambda_ = 0.1\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = w_initial\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_newton_method(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 5 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criteria\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2])/np.abs(losses[-1]) < threshold:\n",
    "            print(\"[Exit Condition Met]: Current iteration={i}, the loss={l}\".format(i=iter, l=loss)) \n",
    "            break\n",
    "    return losses, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=335812.93056121364\n",
      "Current iteration=5, the loss=158320.91478743535\n",
      "Current iteration=10, the loss=137099.26613674927\n",
      "Current iteration=15, the loss=129253.20563647522\n",
      "Current iteration=20, the loss=125994.86967928459\n",
      "Current iteration=25, the loss=124644.98870635527\n",
      "Current iteration=30, the loss=124104.69657472218\n",
      "Current iteration=35, the loss=123896.52072155419\n",
      "Current iteration=40, the loss=123818.86473412794\n",
      "Current iteration=45, the loss=123790.58377213465\n",
      "Current iteration=50, the loss=123780.45149255183\n",
      "Current iteration=55, the loss=123776.85952151449\n",
      "Current iteration=60, the loss=123775.59449955265\n",
      "Current iteration=65, the loss=123775.15077058722\n",
      "Current iteration=70, the loss=123774.99550104163\n",
      "Current iteration=75, the loss=123774.94124766982\n",
      "Current iteration=80, the loss=123774.9223070695\n",
      "Current iteration=85, the loss=123774.91569801522\n",
      "Current iteration=90, the loss=123774.91339257518\n",
      "Current iteration=95, the loss=123774.91258851082\n",
      "Current iteration=100, the loss=123774.91230810824\n",
      "Current iteration=105, the loss=123774.91221032909\n",
      "Current iteration=110, the loss=123774.91217623379\n",
      "Current iteration=115, the loss=123774.91216434514\n",
      "Current iteration=120, the loss=123774.91216019976\n",
      "Current iteration=125, the loss=123774.91215875432\n",
      "Current iteration=130, the loss=123774.91215825033\n",
      "Current iteration=135, the loss=123774.91215807461\n",
      "Current iteration=140, the loss=123774.91215801332\n",
      "Current iteration=145, the loss=123774.91215799196\n",
      "Current iteration=150, the loss=123774.91215798451\n",
      "Current iteration=155, the loss=123774.9121579819\n",
      "Current iteration=160, the loss=123774.912157981\n",
      "Current iteration=165, the loss=123774.91215798068\n",
      "Current iteration=170, the loss=123774.91215798058\n",
      "[Exit Condition Met]: Current iteration=170, the loss=123774.91215798058\n"
     ]
    }
   ],
   "source": [
    "w0 = np.random.rand(tX.shape[1]+1,1)\n",
    "losses, w_star = logistic_regression_newton_method_demo(y, tX, w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tX_pred = np.c_[np.ones((y.shape[0], 1)), tX]\n",
    "prediction = sigmoid(tX_pred.dot(w_star))\n",
    "prediction = np.array( [ int(x) for x in (prediction > 0.5)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "ax=f.add_subplot(2,2,1)\n",
    "ax.scatter(tX[:,0],tX[:,1], c=y_colors, alpha=0.1)\n",
    "\n",
    "ax=f.add_subplot(2,2,2)\n",
    "ax.scatter(tX[:,2],tX[:,3], c=y_colors, alpha=0.1)\n",
    "\n",
    "prediction_colors = np.array(['b']*N_SAMPLES)\n",
    "prediction_colors[prediction==0] = 'r'\n",
    "\n",
    "ax=f.add_subplot(2,2,3)\n",
    "ax.scatter(tX[:,0],tX[:,1], c=prediction_colors, alpha=0.1)\n",
    "\n",
    "ax=f.add_subplot(2,2,4)\n",
    "ax.scatter(tX[:,2],tX[:,3], c=prediction_colors, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 ..., 0 1 0]\n",
      "[ 1.  0.  0. ...,  1.  0.  0.]\n",
      "61900.0\n",
      "0.2476\n"
     ]
    }
   ],
   "source": [
    "error = np.sum(np.abs(prediction - y))\n",
    "print(prediction)\n",
    "print(y)\n",
    "print(error)\n",
    "print(error/N_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3k]",
   "language": "python",
   "name": "conda-env-py3k-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
