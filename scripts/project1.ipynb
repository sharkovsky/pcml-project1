{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import math\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from helpers import *\n",
    "\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "y, X, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to obtain column names of data, and its name -> index mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DER_mass_MMC', 'DER_mass_transverse_met_lep', 'DER_mass_vis',\n",
       "       'DER_pt_h', 'DER_deltaeta_jet_jet', 'DER_mass_jet_jet',\n",
       "       'DER_prodeta_jet_jet', 'DER_deltar_tau_lep', 'DER_pt_tot',\n",
       "       'DER_sum_pt', 'DER_pt_ratio_lep_tau', 'DER_met_phi_centrality',\n",
       "       'DER_lep_eta_centrality', 'PRI_tau_pt', 'PRI_tau_eta',\n",
       "       'PRI_tau_phi', 'PRI_lep_pt', 'PRI_lep_eta', 'PRI_lep_phi',\n",
       "       'PRI_met', 'PRI_met_phi', 'PRI_met_sumet', 'PRI_jet_num',\n",
       "       'PRI_jet_leading_pt', 'PRI_jet_leading_eta', 'PRI_jet_leading_phi',\n",
       "       'PRI_jet_subleading_pt', 'PRI_jet_subleading_eta',\n",
       "       'PRI_jet_subleading_phi', 'PRI_jet_all_pt'], \n",
       "      dtype='<U27')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_x = np.genfromtxt(DATA_TRAIN_PATH, delimiter=\",\", skip_header=0, dtype=str)\n",
    "columns = str_x[0, 2:]\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PRI_lep_pt': 16, 'PRI_tau_eta': 14, 'DER_deltaeta_jet_jet': 4, 'DER_lep_eta_centrality': 12, 'PRI_jet_all_pt': 29, 'PRI_tau_pt': 13, 'PRI_jet_leading_phi': 25, 'DER_pt_tot': 8, 'PRI_jet_subleading_pt': 26, 'PRI_jet_leading_eta': 24, 'PRI_tau_phi': 15, 'DER_pt_h': 3, 'PRI_lep_eta': 17, 'PRI_met_phi': 20, 'DER_deltar_tau_lep': 7, 'DER_mass_transverse_met_lep': 1, 'DER_met_phi_centrality': 11, 'DER_mass_jet_jet': 5, 'PRI_jet_num': 22, 'PRI_met': 19, 'DER_sum_pt': 9, 'DER_prodeta_jet_jet': 6, 'PRI_lep_phi': 18, 'PRI_jet_subleading_eta': 27, 'DER_pt_ratio_lep_tau': 10, 'PRI_jet_subleading_phi': 28, 'DER_mass_vis': 2, 'PRI_jet_leading_pt': 23, 'DER_mass_MMC': 0, 'PRI_met_sumet': 21}\n"
     ]
    }
   ],
   "source": [
    "col_idx = {}\n",
    "for i in range(len(columns)):\n",
    "    col_idx[columns[i]] = i\n",
    "print(col_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data\n",
    "\n",
    "We throw away data whose DER_mass_MMC is -999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def valid_mass_indices(X):\n",
    "    \"\"\"returns a list of row indices where DER_mass_MMC field has valid value\"\"\"\n",
    "    return np.where(X[:, col_idx['DER_mass_MMC']] != -999)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reset_invalid_val(X):\n",
    "    \"\"\"replaces -999 with 0, as 0 will have less bad effects on the training result\"\"\"\n",
    "    for col in range(X.shape[1]):\n",
    "        invalid_idx = np.where(X[:, col] == -999)[0]\n",
    "        X[invalid_idx, col] = 0\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(y, tx):\n",
    "    \"\"\"clean data\"\"\"\n",
    "\n",
    "    # throw away rows with invalid mass value\n",
    "    idx = valid_mass_indices(tx)\n",
    "    tx = tx[idx, :]\n",
    "    y = y[idx]\n",
    "    \n",
    "    # replace -999 with 0\n",
    "    tx = reset_invalid_val(tx)\n",
    "    \n",
    "    # tx = delete_derived_features(tx)\n",
    "    \n",
    "    return y, tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y, tX = clean_data(y, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211886, 30)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211886,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# General gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# general gradient descent\n",
    "def gradient_descent(y, tx, initial_x, gamma, max_iters,\n",
    "                     compute_gradient, compute_loss):\n",
    "    \"\"\"\n",
    "    General gradient descent algorithm\n",
    "    \n",
    "    Params:\n",
    "        y (array): training values\n",
    "        tx (array): each row contains the data associated to a sample.\n",
    "                    each column contains all the sample value for a feature\n",
    "        initial_w (array): the initial weight vector\n",
    "        gamma (float): step size\n",
    "        max_iters (int): maximum number of iterations to run\n",
    "        compute_gradient (func): a function that computes gradient\n",
    "        compute_less (func): a function that computes loss\n",
    "\n",
    "    Returns:\n",
    "        w (array): the last weight vector\n",
    "        loss (float): the last loss value\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        w = w - gamma * gradient\n",
    "\n",
    "        # if (n_iter % 100 == 0):\n",
    "        #    print(loss)\n",
    "        \n",
    "        losses.append(loss)\n",
    "    \n",
    "    return w, losses[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, gamma, max_iters, seed,\n",
    "                                compute_gradient, compute_loss):\n",
    "    \"\"\"\n",
    "    General stochastic gradient descent algorithm\n",
    "    \n",
    "    Params:\n",
    "        y (array): training values\n",
    "        tx (array): each row contains the data associated to a sample.\n",
    "                    each column contains all the sample value for a feature\n",
    "        batch_size (int): batch size\n",
    "        initial_w (array): the initial weight vector\n",
    "        gamma (float): step size\n",
    "        max_iters (int): maximum number of iterations to run\n",
    "        seed (int): random number generation seed\n",
    "        compute_gradient (func): a function that computes gradient\n",
    "        compute_less (func): a function that computes loss\n",
    "\n",
    "    Returns:\n",
    "        w (array): the last weight vector\n",
    "        loss (float): the last loss value\n",
    "    \"\"\"\n",
    "    \n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    num_batches = math.floor(y.shape[0] / batch_size) \n",
    "    batches = batch_iter(y, tx, seed, batch_size, num_batches)\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        batch_y, batch_tx = next(batches)\n",
    "        gradient = compute_gradient(batch_y, batch_tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "\n",
    "        # if (n_iter % 100 == 0):\n",
    "        #    print(loss)\n",
    "        \n",
    "        w = w - gamma * gradient\n",
    "        \n",
    "        losses.append(loss)\n",
    "        \n",
    "    return w, losses[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cost functions\n",
    "def calculate_mse(e):\n",
    "    \"\"\"Calculate the mse for vector e.\"\"\"\n",
    "    return 1/2*np.mean(e**2)\n",
    "\n",
    "def calculate_mae(e):\n",
    "    \"\"\"Calculate the mae for vector e.\"\"\"\n",
    "    return np.mean(np.abs(e))\n",
    "\n",
    "def compute_loss_mse(y, tx, w):\n",
    "    \"\"\"Calculate the loss using mse \"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    return calculate_mse(e)\n",
    "\n",
    "def compute_gradient_mse(y, tx, w):\n",
    "    \"\"\" compute the gradient associated to the MSE cost function\"\"\"\n",
    "    e = y - (tx @ w)\n",
    "    return -1/y.shape[0] * (tx.T @ e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, gamma, max_iters):\n",
    "    return gradient_descent(y, tx, initial_w, gamma, max_iters,\n",
    "                            compute_gradient_mse, compute_loss_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iters = 1000\n",
    "gamma = 1e-6\n",
    "initial_w = np.zeros(tX.shape[1])\n",
    "\n",
    "w_linear_gd, loss_linear_gd = least_squares_GD(y, tX, initial_w, gamma, max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40265042191731415"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_linear_gd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using stochastic gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, gamma, max_iters):\n",
    "    batch_size = y.shape[0]//2\n",
    "    seed = 3\n",
    "    \n",
    "    return stochastic_gradient_descent(y, tx, initial_w, batch_size, gamma, max_iters,\n",
    "                                       seed, compute_gradient_mse, compute_loss_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iters = 5000\n",
    "gamma = 4e-7\n",
    "initial_w = np.zeros(tX.shape[1])\n",
    "\n",
    "w_linear_sgd, loss_linear_sgd = least_squares_SGD(y, tX, initial_w, gamma, max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3985179351390859"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_linear_sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    w = (np.linalg.inv(tx.T @ tx) @ tx.T @ y)\n",
    "    loss = compute_loss_mse(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_ls, loss_ls = least_squares(y, tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37717616465985404"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.96490590e-03,  -8.44190753e-03,  -7.26535537e-03,\n",
       "         1.07588572e-03,  -1.59616405e-03,   3.99733431e-04,\n",
       "        -2.96780330e-03,   1.36877265e-01,  -1.59044104e-03,\n",
       "        -4.95865902e+00,  -2.98661958e-01,   7.19197758e-02,\n",
       "         3.61840394e-01,   4.96119931e+00,  -1.08333759e-03,\n",
       "        -9.52686303e-04,   4.96974908e+00,  -6.56814462e-04,\n",
       "         1.26463254e-03,  -3.14363964e-04,   4.87948602e-04,\n",
       "        -6.63146763e-04,  -7.43474417e-02,   4.79290812e-04,\n",
       "         9.72092505e-04,   1.03122590e-03,   2.23111208e-04,\n",
       "         1.61676265e-03,  -2.60772556e-03,   4.95769084e+00])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    w = np.linalg.inv(tx.T @ tx + lambda_ * np.identity(tx.shape[1])) @ tx.T @ y\n",
    "    loss = compute_loss_mse(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lamb = 23\n",
    "w_ridge, loss_ridge = ridge_regression(y, tX, lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37717992915741932"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1 / (1 + np.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss_log(y, tx, w):\n",
    "    \"\"\"compute cost by negative log likelihood.\"\"\"\n",
    "    xw = tx @ w\n",
    "    return np.sum(np.log(1 + np.exp(xw)) - y * xw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_gradient_log(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    xw = tx @ w\n",
    "    return tx.T @ (np.apply_along_axis(sigmoid, 0, xw) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, gamma, max_iters):\n",
    "    \"\"\"logistic regression algorithm using gradient descent\"\"\"\n",
    "    return gradient_descent(y, tx, initial_w, gamma, max_iters,\n",
    "                            compute_gradient_log, compute_loss_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iters = 800\n",
    "gamma = 3e-13\n",
    "initial_w = np.zeros(tX.shape[1])\n",
    "\n",
    "w_log, loss_log = logistic_regression(y, tX, initial_w, gamma, max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-60469.70129449368"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized  logistic  regression  using  gradient  descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss_rlog(y, tx, w, lambda_):\n",
    "    return compute_loss_log(y, tx, w) + lambda_ * w.T @ w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_gradient_rlog(y, tx, w, lambda_):\n",
    "    return compute_gradient_log(y, tx, w) + 2 * lambda_ * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reg_logistic_regression(y, tx, lambda_, initial_w, gamma, max_iters):\n",
    "    \"\"\"regularized logistic regression algorithm using gradient descent\"\"\"\n",
    "    compute_gradient = partial(compute_gradient_rlog, lambda_ = lambda_)\n",
    "    compute_loss = partial(compute_loss_rlog, lambda_ = lambda_)\n",
    "    return gradient_descent(y, tx, initial_w, gamma, max_iters,\n",
    "                            compute_gradient, compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iters = 1000\n",
    "gamma = 1e-13\n",
    "# initial_w = np.random.rand(tX.shape[1])#\n",
    "initial_w = np.zeros(tX.shape[1])\n",
    "\n",
    "lambdas = np.logspace(-4, 2, 30)\n",
    "\n",
    "w_rlog, loss_rlog = reg_logistic_regression(y, tX, lambdas, initial_w, gamma, max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15438.02691224187"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_rlog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tX_test = reset_invalid_val(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submission.csv'\n",
    "y_pred = predict_labels(w_ridge, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3k]",
   "language": "python",
   "name": "conda-env-py3k-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
